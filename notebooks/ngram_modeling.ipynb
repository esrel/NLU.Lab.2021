{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Language Modeling\n",
    "\n",
    "- Natural Language Understanding\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)) is advised for reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sections *Ngrams and Ngram Probabilities* and *Language Models* roughly cover *Chapter 3: \"N-gram Language Models\"*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Requirements__\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
    "\n",
    "    - run `git clone https://github.com/esrel/NL2SparQL4NLU.git`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 0. Ngrams and Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of *n* items from a given sequence of text or speech. An n-gram model models sequences, notably natural languages, using the statistical properties of n-grams.\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character n-grams: cat\n",
    "- word n-grams: the cat is fat\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['c', 'a', 't']` | `['ca', 'at']` | `['cat']` |\n",
    "| *Word N-grams*      | `['the', 'cat', 'is' , 'fat']` | `['the cat', 'cat is', ...]` | `['the cat is', ...]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count. Ngram count only differs in a unit of counting -- sequence of $n$ of tokens. We can compute bigram count by taking sequences of 2 items, trigrams - 3, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.1.1. Data Structures\n",
    "For *Frequency List* we have used [`dict`](https://docs.python.org/2/library/stdtypes.html#dict) to store counts.\n",
    "In ngram counting scenario we still can use dictionary, and use ngram string keys; however, there are better data structures for the task. The frequent data structures used to store ngram counts or probabilities are [hash table](https://en.wikipedia.org/wiki/Hash_table), [trie](https://en.wikipedia.org/wiki/Trie), or [finite state automaton](https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton). The pros and cons of each data structure are out of the scope of this lab; for the discussion on efficient data structures for language modeling please refer to [KenLM paper](https://kheafield.com/papers/avenue/kenlm.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.1.2. Implementing Trie\n",
    "There are plenty of implementation of the Trie data structure (e.g. [`pygtrie`](https://github.com/google/pygtrie/) from Google). However, it is simple enough. \n",
    "\n",
    "For understanding purposes let's implement our own, such that:\n",
    "\n",
    "- works on lists of words\n",
    "- stores in a node: \n",
    "    - word\n",
    "    - its count\n",
    "    - children (next words)\n",
    "- implements methods to:\n",
    "    - add a sequence (updating counts)\n",
    "    - get a node by a sequence (i.e. prefix)\n",
    "    - traverse a trie and get all sequences\n",
    "        - allows to traverse children of any node\n",
    "    - compute size of ngram vocabulary (V)\n",
    "    \n",
    "It is convenient to introduce an `oov` node to the Trie to easily handle sequences not in it.\n",
    "Thus, we also add `self.ovv` (with `count` set to $0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, word=None):\n",
    "        self.word = word\n",
    "        self.children = {}\n",
    "        self.count = 0\n",
    "\n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "\n",
    "class Trie(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = Node('*')  # trie root\n",
    "        self.oov = Node()      # node for oov values\n",
    "        self.size = 0          # depth of trie\n",
    "\n",
    "    def __set__(self, instance, value):\n",
    "        self.instance = value\n",
    "\n",
    "    def __get__(self, instance, owner):\n",
    "        return self.instance\n",
    "\n",
    "    def add(self, sequence):\n",
    "        node = self.root\n",
    "        node.count += 1  # total word count\n",
    "        for word in sequence:\n",
    "            node.children[word] = node.children.setdefault(word, Node(word))\n",
    "            node = node.children[word]\n",
    "            node.count += 1\n",
    "\n",
    "    def get(self, sequence):\n",
    "        node = self.root\n",
    "        for word in sequence:\n",
    "            node = node.children.get(word, self.oov)\n",
    "        return node\n",
    "\n",
    "    def traverse(self, node=None, sequence=None, size=None):\n",
    "        sequence = sequence if sequence else []\n",
    "        node = self.root if not node else node\n",
    "\n",
    "        if not node.children:\n",
    "            yield sequence\n",
    "\n",
    "        if size:\n",
    "            if len(sequence) == size:\n",
    "                yield sequence\n",
    "\n",
    "        for word, n in node.children.items():\n",
    "            sequence.append(word)\n",
    "            yield from self.traverse(n, sequence, size=size)\n",
    "            sequence.pop()\n",
    "\n",
    "    def v(self, size=None):\n",
    "        return len(list(self.traverse(size=size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing out the implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram size: 4\n",
      "3 ['a']\n",
      "2 ['a', 'b']\n",
      "0 ['a', 'x']\n",
      "0 ['e', 'c', 'f']\n",
      "1 ['a', 'e', 'c', 'f']\n",
      "All In Trie:\n",
      "['a', 'b', 'c', 'd']\n",
      "['a', 'b', 'g', 'h']\n",
      "['a', 'e', 'c', 'f']\n",
      "['x', 'y', 'z', 'a']\n",
      "In Trie for: ['a']\n",
      "1 ['a', 'b', 'c', 'd']\n",
      "1 ['a', 'b', 'g', 'h']\n",
      "1 ['a', 'e', 'c', 'f']\n",
      "1-gram V: 6\n",
      "2-gram V: 7\n",
      "3-gram V: 8\n",
      "4-gram V: 8\n"
     ]
    }
   ],
   "source": [
    "counts = Trie()\n",
    "\n",
    "# adding 4-grams\n",
    "counts.add(['a', 'b', 'c', 'd'])\n",
    "counts.add(['a', 'e', 'c', 'f'])\n",
    "counts.add(['a', 'b', 'g', 'h'])\n",
    "counts.add(['x', 'y', 'z', 'a'])\n",
    "\n",
    "# setting & getting meta-info\n",
    "counts.size = 4\n",
    "print('ngram size:', counts.size)\n",
    "\n",
    "# testing counts for n-grams of various sizes\n",
    "tests = [['a'], ['a', 'b'], ['a', 'x'], ['e', 'c', 'f'], ['a', 'e', 'c', 'f']]\n",
    "\n",
    "# getting counts\n",
    "for seq in tests:\n",
    "    print(counts.get(seq).count, seq)\n",
    "\n",
    "# traversing trie: getting all strings\n",
    "print(\"All In Trie:\")\n",
    "for seq in counts.traverse():\n",
    "    print(seq)\n",
    "\n",
    "# traversing trie: by prefix (['a'])\n",
    "print(\"In Trie for: {}\".format(['a']))\n",
    "for seq in counts.traverse(node=counts.get(['a']), sequence=['a']):\n",
    "    print(counts.get(seq).count, seq)\n",
    "    \n",
    "# getting size of ngram vocabulary\n",
    "for i in range(counts.size):\n",
    "    print(\"{}-gram V: {}\".format(i+1, counts.v(size=i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to extract ngrams (variable $n$) from a sequence (sentence) represented as a list\n",
    "- define a function to compute ngram counts from a corpus (list-of-lists) and store as a Trie\n",
    "- compute bigram counts for the training set of NL2SparQL4NLU\n",
    "- report `5` most frequent bigrams comparing to the reference values below (you can use `nbest` on `node.children`)\n",
    "\n",
    "\n",
    " word 1 | word 2 | count \n",
    ":-------|:-------|-------:\n",
    "show    | me     |   377\n",
    "the     | movie  |   267\n",
    "of      | the    |   186\n",
    "me      | the    |   122\n",
    "is      | the    |   120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Computing Ngram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.2.1. Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "- define a function to compute ngram probabilities using ngram counts\n",
    "\n",
    "- compute probabilities of ngrams ($n=2$) in the training set of NL2SparQL4NLU\n",
    "- report probabilities of the following ngrams\n",
    "    - $p(the | of)$\n",
    "    - $p(the | is)$\n",
    "    - $p(play | the)$\n",
    "    - probabilities of all bigram where $w_1$ is \"*italy*\", i.e. $p(*|italy)$\n",
    "\n",
    "\n",
    "| ngram            | $$\\approx p$$ | $$\\approx\\log(p)$$ |\n",
    "|:-----------------|--------------:|-------------------:|\n",
    "| $$p(the|of)$$    | 0.31          | -1.18     \n",
    "| $$p(the|is)$$    | 0.36          | -1.03\n",
    "| $$p(play|the)$$  | 0.00          | \n",
    "| $$p(make|italy)$$| 0.50          |\n",
    "| $$p(in|italy)$$  | 0.50          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.2.2. Underflow Problem\n",
    "\n",
    "Probabilities are usually small ($<1$).\n",
    "Multiplying many of those may cause *underflow* problem.\n",
    "\n",
    "Use the sum of the probabilities' logs instead of product\n",
    "\n",
    "| Properties     \n",
    "|:---------------\n",
    "| $$p(a) > p(b)$$\n",
    "| $$log(p(a)) > log(p(b))$$\n",
    "| $$log(a*b) = log(a) + log(b)$$\n",
    "| $$p(a) * p(b) \\rightarrow log(p(a)) + log(p(b))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "- update the function to compute probabilities to use log (use `math` library)\n",
    "- define a function to convert log probabilities to probabilities (`exp()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $p(w_{1},\\ldots ,w_{n})$ ($p(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1.1. Probability of a Sequence: [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities (chain rule). \n",
    "\n",
    "$$p(w_{1}^{n}) = p(w_1) p(w_2|w_1) p(w_3|w_1^2) ... p(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{p(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$p(w_i|w_{1}^{i-1}) \\approx p(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\n",
    "N-gram   | Equation                   |\n",
    ":--------|:---------------------------|\n",
    "unigram  | $$p(w_i)$$                 |\n",
    "bigram   | $$p(w_i|w_{i-1})$$         |\n",
    "trigram  | $$p(w_i|w_{i-2},w_{i-1})$$ |\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$p(w_{1}^{n}) = \\prod_{i=1}^{n}{p(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1.2. Sentence beginning & end tags\n",
    "\n",
    "Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence tag, on the other hand, makes the bigram model a true probability distribution (Jurafsky and Martin). \"Without it, the sentence probabilities for all sentences of a given length would sum to one. This model would define an infinite set of probability distributions, with one distribution per sentence length.\"\n",
    "\n",
    "For larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Example__: \"The cat is fat\"\n",
    "\n",
    "$p($ `<s>`, the, cat, is, fat, `</s>` $) = p(the |$ `<s>` $) * p(cat | the) * p(is | cat) * p(fat | is) * p($ `</s>` $| fat)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "- define a function to add sentence beginning and end tags to a corpus as list-of-lists\n",
    "- define a function to compute sentence probability given an ngram model\n",
    "    - remember that for log we use sum for raw probabilities product\n",
    "        - use `math.prod` for Python 3.8\n",
    "        - use `numpy.prod` for Python < 3.8\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU\n",
    "- compute probability of sentences: *star of twilight* and *star of thor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = $ `<s>`;\n",
    "- *while* $w_i \\neq $ `</s>`\n",
    "\n",
    "    - stochastically get new word w.r.t. $p(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- define a function to generate random sentences (e.g. using `random.choice`)\n",
    "- generate `5` different sentences & score them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3. Evaluation: [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\n",
    "$$ PPL = \\sqrt[N]{\\frac{1}{p(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}p(w_i|w_{i-N+1})}}$$\n",
    "\n",
    "Where $N$ is the number of words in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4. Handling Unseen Words\n",
    "\n",
    "Out-Of-Vocabulary (OOV) word -- tokens in test data that are not contained in the lexicon (vocabulary).\n",
    "Empirically each OOV word results in 1.5 - 2 extra errors (> 1 due to the loss of contextual information).\n",
    "\n",
    "__*How to handle words (in test set) that were never seen in the training data?*__\n",
    "\n",
    "Train a language model with specific token (e.g. `<unk>`) for unknown words!\n",
    "\n",
    "__*How to estimate probabilities of unknown words and ngrams?*__\n",
    "\n",
    "The *simplest* approach is to replace all the words that are not in vocabulary (lexicon) with the `<unk>` token and treat it as any other word. (For instance, applying frequency cut-off to the lexicon, will allow estimate these probabilities on the training set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- define a function to replace OOV words in a corpus as list-of-list given a lexicon\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU (with `<unk>`)\n",
    "- re-compute probability of sentences: *star of twilight* and *star of thor*\n",
    "    - replace unknown words and add tags as well\n",
    "- compare scores to the ones without unknown word handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.6. Handling Data Sparseness\n",
    "\n",
    "What do we do with words that are in our lexicon, but appear in a test set in an unseen context (i.e. no ngram)?\n",
    "\n",
    "Similar to unseen words and unseen n-grams have $0$ probability; thus, whole sequence gets $0$ probability.\n",
    "(The problem is somewhat avoided using log probabilities.)\n",
    "\n",
    "Use smoothing:\n",
    "- Add some probability to unseen events\n",
    "- Remove some probability from seen events\n",
    "- Joint probability distribution sums to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Smoothing Methods\n",
    "\n",
    "Available smoothing methods: ([tutorial](https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf))\n",
    "- [Additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) (__simplest__)\n",
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation)\n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.6.1. Add-One Smoothing\n",
    "Kind of Additive Smoothing.\n",
    "\n",
    "__Bigrams__\n",
    "\n",
    "$V$ -- vocabulary size\n",
    "\n",
    "$$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$\n",
    "\n",
    "__N-grams__\n",
    "\n",
    "$V$ -- total number of possible $(N-1)$-grams\n",
    "\n",
    "$$p(w_i | w^{i-1}_{i-N+1}) = \\frac{c(w^{i-1}_{i-N+1},w_i)+1}{c(w^{i-1}_{i-N+1})+V}$$\n",
    "\n",
    "Typically, we assume $V = \\{w : c(w) > 0\\} \\cup \\{$ `<unk>` $\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- update the ngram probability calculation function to apply add one smoothing\n",
    "- re-compute bigram probabilities for the training set of NL2SparQL4NLU with smoothing\n",
    "- compare scores to the ones without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.6.2. Interpolation (Jelinek-Mercer Smoothing)\n",
    "[Interpolation](https://en.wikipedia.org/wiki/Interpolation) estimates the probability by combining more robust, but weaker estimators; i.e. fall-back to lower order ngram probabilities to estimate a higher ngram probability. \n",
    "The standard way of combination is to use *linear interpolation*: to estimate a probability of a trigram, we use a weighted sum of unigrams, bigrams, and trigram probabilities as:\n",
    "\n",
    "$$p(w_i|w_{i-1}, w_{i-2}) = \\lambda_3 p(w_i|w_{i-1},w_{i-2}) + \\lambda_2 p(w_i|w_{i-1}) + \\lambda_1 p(w_i)$$\n",
    "\n",
    "Where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n",
    "\n",
    "> For any size of ngram: $n^{th}$-order smoothed model is defined recursively as a linear interpolation between the $n^{th}$-order Maximum Likelihood (ML) model and the $(n−1)^{th}$-order smoothed model.\n",
    "\n",
    "$$p_{INT}(w_i|w_{i-N+1}^{i-1}) = \\lambda_{w_{i-N+1}^{i-1}} p_{ML}(w_i|w_{i-N+1}^{i-1}) + (1-\\lambda_{w_{i-N+1}^{i-1}}) p_{INT}(w_i|w_{i-N+2}^{i-1})$$\n",
    "\n",
    "The recursion can be grounded as:\n",
    "- unigram model: $p_{ML}(w_i)$\n",
    "- uniform distribution (e.g. for OOV)\n",
    "\n",
    "$$p_{U}(w_i)=\\frac{1}{V}$$\n",
    "\n",
    "Values of $\\lambda$s are computed using *__deleted interpolation__*: \n",
    "\n",
    "> we successively delete each trigram from the training corpus and choose the $\\lambda$s so as to maximize the likelihood of the rest of the corpus (similar to leave-one-out cross-validation). \n",
    "\n",
    "The deletion helps to set the $\\lambda$s in a way to prevent over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def deleted_interpolation(counts):\n",
    "    \"\"\"\n",
    "    generic deleted interpolation\n",
    "    :param counts: counts trie\n",
    "    :return: interpolation weights for ngram models\n",
    "    \"\"\"\n",
    "    w = [0] * counts.size\n",
    "    for ngram in counts.traverse():\n",
    "        # current ngram count\n",
    "        v = counts.get(ngram).count\n",
    "        # (n)-gram counts\n",
    "        n = [counts.get(ngram[0:i+1]).count for i in range(len(ngram))]\n",
    "        # (n-1)-gram counts -- parent node\n",
    "        p = [counts.get(ngram[0:i]).count for i in range(len(ngram))]\n",
    "        # -1 from both counts & normalize\n",
    "        d = [float((n[i]-1)/(p[i]-1)) if (p[i]-1 > 0) else 0.0 for i in range(len(n))]\n",
    "        # increment weight of the max by raw ngram count\n",
    "        k = d.index(max(d))\n",
    "        w[k] += v\n",
    "    return [float(v)/sum(w) for v in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.7. Putting it all together (Exercise)\n",
    "\n",
    "Train an Ngram Language Model (compute ngram probabilities) such that:\n",
    "\n",
    "- case insensitive (by default)\n",
    "- 2-gram\n",
    "- log probabilities\n",
    "- considers sentence boundaries (beginning and end of sentence tags)\n",
    "- considers unknown words\n",
    "- Add-One Smoothing\n",
    "\n",
    "Compute probabilities of utterances in `NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Cache Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kuhn and De Mori (1990) \"Cache-based natural language model for speech recognition\"\n",
    "\n",
    "> The main limitation of the Markov models is their inability to reflect short-term patterns in word use.\n",
    "\n",
    "A [cache language model](https://en.wikipedia.org/wiki/Cache_language_model) is a type of statistical language model. The main difference of a cache langauge model is the existence of a cache component that stores recently observed or most frequent words or ngrams. The contents of a cache are assigned relatively high probabilities with respect to the rest of sequences.\n",
    "\n",
    "The basic idea of a cache LM is to interpolate a _static_ LM estimated from a large amount of data with a _dynamic_ LM estimated from recently observed words in the document being processed:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p(w_i|w_{i-N+1}^{i-1}) = \\lambda p_{static}(w_i|w_{i-N+1}^{i-1}) + (1 − \\lambda) p_{cache}(w_i|w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Commonly, the cache stores a limited amount of data; thus, the higher-order n-grams are not used. However, it was observed that bigrams are often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation Questions\n",
    "- How to window text for building the cache and for temporally weighting data (higher weights for recent words)\n",
    "- What to store in change (e.g. function words are not updated, but this leads to trickier normalization)\n",
    "- How to tune $\\lambda$ (e.g. heuristically to minimize extrinsic metric such as Word Error Rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Simple Cache LM\n",
    "\n",
    "- Let $K$ be the cache size\n",
    "- Let $w_i$ be the $i$−th word in a text and let $h = w_{i−K}, ..., w_{i−1}$ denote the cache or history of $w_{i}$\n",
    "\n",
    "Then, $p_{cache}(w_i|h)$ can be estimates as:\n",
    "\n",
    "$$ p_{cache}(w_i|h) = \\frac{C(w_i, h)}{C(h)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C(h)$ is the number of words within $h$ that belong vocabulary $V$ (depends on how cache is updated).\n",
    "- $C(w_i, h)$ is the number of occurrences of a word $w_i$ within $h$.\n",
    "\n",
    "For an arbitrary n-gram size cache LM, the equation becomes:\n",
    "\n",
    "$$ p_{cache}(w_i|w_{i-N+1}^{i-1}, h) = \\frac{C(w_i, w_{i-N+1}^{i-1}, h)}{C(w_{i-N+1}^{i-1}, h)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Exercise\n",
    "\n",
    "- Update LM implementation with cache\n",
    "- Evaluate both versions on the test set using perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kneser-Ney Smoothing [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_{KN}(w_i|w^{i-1}_{i-N+1}) = \\frac{max(C_{KN}(w^{i}_{i-N+1} - d, 0))}{\\sum_{v} C_{KN}(w_{i-N+1}^{i-1} v)} + \\lambda(w_{i-N+1}^{i-1}) p_{KN}(w_{i-N+2}^{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$C_{KN}(\\cdot) = \\left\\{ \\begin{array}{@{}l} \\text{count}(\\cdot) \\text{ for the highest order}\\\\\\text{continuation_count}(\\cdot) \\text{ for lower orders} \\end{array} \\right.$$\n",
    "\n",
    "The $\\texttt{continuation count}$ is the number of unique single word contexts for $\\cdot$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At the termination of the recursion, unigrams are interpolated with the uniform distribution, where the parameter $\\epsilon$ is the empty string: \n",
    "\n",
    "$$ p_{KN}(w) = \\frac{max(C_{KN}(w) - d, 0)}{\\sum_{w'} C_{KN}(w′)} + \\lambda(\\epsilon)\\frac{1}{V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
